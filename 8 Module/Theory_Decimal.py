#Комп'ютер усі обчислення робить в бінарному вигляді, 
# а десяткові числа використовуються тільки для "спілкування" з користувачем для зручності останнього.

#Через це і через те, що точність обчислень в комп'ютері обмежена, 
# виникають помилки округлення під час виконання математичних операцій.
print(0.1 + 0.2 == 0.3)     # False
print(0.1 + 0.2)            # 0.30000000000000004

# Перший вираз може збити вас з пантелику, оскільки математика стверджує однозначно, що 0.1 + 0.2 = 0.3. 
# Але помилка округлення під час виконання обчислювальних операцій з дійсними числами 
# у двійковій системі обчислення призводить до такої неоднозначності.

# Проблема точності обчислень так і не розв'язана остаточно, і математика продовжує розвиватися в цьому напрямі.
# Щоб контролювати точність обчислень більш явно, у Python є пакет decimal.
getcontext().prec = 6
Decimal(1) / Decimal(7)  # Decimal('0.142857')

getcontext().prec = 28
Decimal(1) / Decimal(7) # Decimal('0.1428571428571428571428571429')
#У цьому прикладі ми вирахували вираз 1 / 7 з точністю до 6 знаків після коми і до 28 знаків. 

# Щоб встановити точність обчислення, ми скористалися функцією getcontext, 
# яка повертає поточні налаштування точності, та встановили налаштування prec у 6 та 28 відповідно.

#Об'єкти Decimal поводяться так само, як float, але їх і не можна використовувати в одному виразі разом.
# Виконання виразу на кшталт Decimal(0.1) + 0.2 призведе до помилки.

#Повертаючись до нашого прикладу із додаванням 0.1 та 0.2:
getcontext().prec = 6
float(Decimal(0.1) + Decimal(0.2)) == 0.3   # True


#На жаль, Decimal має ще ту особливість, 
# що при створенні Decimal із float його точність береться максимальною для цієї платформи,
# а не з налаштувань getcontext.

#Саме тому:

Decimal(0.2) + Decimal(0.1) == Decimal(0.3) # False

#але

Decimal(0.2) + Decimal(0.1) == Decimal(0.3)  + Decimal(0.0) # True
#При конвертації в float точність береться з налаштувань getcontext.